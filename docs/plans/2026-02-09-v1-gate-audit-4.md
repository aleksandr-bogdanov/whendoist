# V1.0 Gate Audit — Fourth Pass

> Opus investigation, February 9 2026. Deep audit of areas not covered by audits #1–3,
> focused on finding unknowns: issues that are neither fixed nor in the Post-v1.0 Backlog.
>
> Method: Full CI run (578 tests, ruff, pyright), PR verification for v0.42.16–18,
> then 8 parallel code investigations covering SQL injection, IDOR, input validation,
> backup/OAuth/cookies/CORS, HTMX/logging/service worker, backup import/race conditions/IDs,
> and encryption/background tasks/migrations/API size/thoughts.

---

## 0. Preconditions — All Green

| Check | Result |
|-------|--------|
| `ruff format .` | 96 files unchanged |
| `ruff check .` | All checks passed |
| `pyright app/` | 0 errors, 0 warnings |
| `just test` | 578 passed, 0 failed |

---

## 1. Sonnet PR Verification — All 3 Confirmed

### PR #88 (v0.42.16) — Sanitize Exception Messages ✅

| Fix | File:Line |
|-----|-----------|
| JSONDecodeError handler — generic message | `backup.py:102` |
| Generic exception handler — generic message | `backup.py:106` |
| Todoist preview — generic message | `import_data.py:211` |
| Passkey registration — generic message | `passkeys.py:177` |
| Passkey authentication — generic message | `passkeys.py:264` |

### PR #89 (v0.42.17) — Self-Host Vendor Scripts, Tighten CSP ✅

| Fix | File:Line |
|-----|-----------|
| Pico CSS → `/static/vendor/pico.min.css` | `base.html:59` |
| Air Datepicker CSS → `/static/vendor/air-datepicker.css` | `base.html:60` |
| HTMX → `/static/vendor/htmx.min.js` | `base.html:95` |
| Air Datepicker JS → `/static/vendor/air-datepicker.js` | `base.html:96` |
| ApexCharts → `/static/vendor/apexcharts.min.js` | `analytics.html:206` |
| `cdn.jsdelivr.net` removed from CSP | `security.py:25-36` |
| 5 vendor files committed to `static/vendor/` | Directory verified |

### PR #90 (v0.42.18) — Clean Up GCal Events on Data Wipe ✅

| Fix | File:Line |
|-----|-----------|
| GCal calendar deletion before task wipe | `import_data.py:94-110` |
| Sync state cleanup (GoogleCalendarEventSync) | `import_data.py:113-117` |
| Graceful exception handling with warning log | `import_data.py:107-110` |

---

## 2. New Findings

### MEDIUM

**M1. Task sheet dead code contains CSRF vulnerabilities and safeFetch bypass**

`app/templates/tasks/_task_sheet.html` is an HTMX-powered task editor that is currently
**dead code** — the required DOM elements (`#task-sheet`, `#sheet-overlay`) are not defined
in any page template. `task-sheet.js:10-11` gets `null` for both, and all functions
early-return.

Despite being dead code, the file contains three CSRF gaps that would go live immediately
if the sheet is activated:

| Issue | File:Line | Detail |
|-------|-----------|--------|
| HTMX form lacks CSRF token | `_task_sheet.html:23` | `hx-post`/`hx-put` to `/api/v1/tasks` — no `X-CSRF-Token` header injected |
| Unschedule uses raw `fetch()` | `_task_sheet.html:145` | `fetch(\`/api/v1/tasks/${taskId}\`, { method: 'PUT' })` — no CSRF, no safeFetch |
| Delete uses raw `fetch()` | `_task_sheet.html:197` | `fetch(\`/api/v1/tasks/${taskId}\`, { method: 'DELETE' })` — no CSRF, no safeFetch |
| Skip uses `getCSRFHeaders()` | `_task_sheet.html:171` | Has CSRF but bypasses `safeFetch()` |
| configRequest only adds nav header | `task-sheet.js:126-131` | Adds `X-Whendoist-Sheet-Nav`, not CSRF |

The HTMX form has a second fatal issue: it sends `application/x-www-form-urlencoded`
but the API expects JSON (`TaskCreate`/`TaskUpdate` are Pydantic models parsed from
JSON body). Even if CSRF passed, FastAPI would return 422.

Additionally, there are zero other `hx-post`/`hx-put`/`hx-delete`/`hx-patch` attributes
in any template — this is the only HTMX state-changing request in the codebase. There is
no global `htmx:configRequest` listener that injects CSRF tokens.

**Recommended action:** Before activating the task sheet, add a global CSRF header
injector for HTMX requests, convert the form to use safeFetch-based JS submission
(matching `task-dialog.js` pattern), and fix the raw `fetch()` calls. Alternatively,
delete the dead code if the sheet UI is not planned.

---

**M2. Pydantic numeric fields missing range validation**

Several Pydantic request models accept unbounded integers. Client-side HTML has
`min`/`max` attributes but the API accepts anything.

| Field | File:Line | Current | Should Be |
|-------|-----------|---------|-----------|
| `TaskCreate.impact` | `tasks.py:136` | `int = 4` | `Field(default=4, ge=1, le=4)` |
| `TaskUpdate.impact` | `tasks.py:189` | `int \| None = None` | `Field(None, ge=1, le=4)` |
| `TaskCreate.duration_minutes` | `tasks.py:135` | `int \| None = None` | `Field(None, ge=1, le=1440)` |
| `TaskUpdate.duration_minutes` | `tasks.py:188` | `int \| None = None` | `Field(None, ge=1, le=1440)` |
| `ScheduledTaskUpdate.duration_minutes` | `api.py:98` | `int` | `Field(ge=1, le=1440)` |
| `ImportOptions.completed_limit` | `import_data.py:60` | `int = 200` | `Field(default=200, ge=0, le=1000)` |

`impact` is used in sorting and Plan My Day scheduling. Invalid values (0, -1, 999)
cause incorrect prioritization. `duration_minutes` is used in GCal event creation —
negative or huge values cause GCal API errors or multi-day events. `completed_limit`
controls Todoist API fetch size — unbounded value triggers excessive external API calls.

**Recommended action:** Add `Field()` constraints. Small change, eliminates class of bugs.

---

### LOW

**L1. Pydantic string fields missing max_length (DB mismatch)**

Several string fields in request models have no `max_length` while the database column
has a fixed-width type. Oversized values would be silently truncated by the DB or cause
a 500 error depending on the database engine.

| Field | File:Line | Pydantic | DB Column |
|-------|-----------|----------|-----------|
| `RegistrationVerifyRequest.name` | `passkeys.py:40` | `str` | `String(100)` |
| `RegistrationVerifyRequest.prf_salt` | `passkeys.py:41` | `str` | `String(64)` |
| `RegistrationVerifyRequest.wrapped_key` | `passkeys.py:42` | `str` | `Text` |
| `EncryptionSetupRequest.salt` | `preferences.py:65` | `str` | `String(64)` |
| `EncryptionSetupRequest.test_value` | `preferences.py:66` | `str` | `Text` |

For `Text` columns, the risk is memory consumption on very large payloads.
For `String(N)` columns, oversized values cause DB-level errors.

**Recommended action:** Add `max_length` to `Field()` or `@field_validator` matching
DB constraints plus reasonable margin.

---

**L2. Collection fields missing size limits**

Batch and list endpoints accept unbounded arrays. A request with 1M items would
consume significant memory during Pydantic validation and DB processing.

| Field | File:Line | Current |
|-------|-----------|---------|
| `BatchUpdateTasksRequest.tasks` | `tasks.py:727` | `list[TaskContentData]` |
| `BatchUpdateDomainsRequest.domains` | `domains.py:189` | `list[DomainContentData]` |
| `CommitTasksRequest.tasks` | `api.py:104` | `list[ScheduledTaskUpdate]` |
| `CalendarSelectionsRequest.calendar_ids` | `api.py:353` | `list[str]` |
| `BatchAction.action` | `instances.py:184` | `str` (should be `Literal["complete", "skip"]`) |

The `BatchAction.action` field uses a raw `str` instead of `Literal` or `Enum`. The
handler validates manually at `instances.py:225-230` with `if/elif/else` → 400, so
there's no security gap, but the contract is comment-based.

**Recommended action:** Add `max_length` (or Pydantic v2 `max_length` on lists) and
convert `BatchAction.action` to `Literal["complete", "skip"]`.

---

**L3. Backup import loses subtask hierarchy**

`backup_service.py:322-342` correctly serializes `parent_id` in the export. But the
import schema `BackupTaskSchema` at `backup_service.py:47-67` omits `parent_id`.
All imported tasks are created as top-level (parent_id=None), silently losing the
parent-child hierarchy with no warning to the user.

**Scenario:** User exports backup with 50 tasks including subtask trees → re-imports →
all 50 tasks appear as top-level with no hierarchy. The data is there but the
relationships are gone. This is especially problematic as a disaster recovery path —
the backup/restore feature gives a false sense of completeness.

**Recommended action:** Fix now. Add `parent_id: int | None = None` to `BackupTaskSchema`,
build an `old_id → new_id` map during import (similar to `domain_id_map`), and
assign `parent_id` after all tasks are created.

---

## 3. Verified Safe — No Issues Found

### SQL Injection ✅

All `text()` calls use static SQL strings with no string interpolation:
- `main.py:84,223` — `text("SELECT 1")` health checks
- `main.py:235` — `text("SELECT COUNT(*) FROM google_tokens ...")` static metric query
- Alembic migrations — all `op.execute()` calls use hardcoded SQL

All application queries use SQLAlchemy ORM with parameterized `.where()` clauses.
No f-string or `.format()` SQL construction anywhere in the codebase.

### IDOR (Beyond user_id) ✅

All ID-based endpoints use single-query ownership checks:
- Tasks: `Task.id == task_id, Task.user_id == self.user_id` (`task_service.py:214-225`)
- Domains: `Domain.id == domain_id, Domain.user_id == self.user_id` (`task_service.py:39`)
- Instances: JOIN on parent Task with `Task.user_id == self.user_id` (`recurrence_service.py:240-246`)
- Passkeys: `UserPasskey.user_id == self.user_id` (`passkey_service.py:262-265`)
- Batch operations: silently skip non-owned IDs (safe pattern)

Parent_id validation on subtask creation: `get_task(parent_id)` validates ownership
before accepting (`task_service.py:249`). Non-owned parent → parent_id becomes None.

All endpoints return 404 uniformly for "doesn't exist" and "not owned" — no information
leak distinguishing the two cases.

### Backup Export Contents ✅

`backup_service.py:312-359` exports only: domain metadata (id, name, icon, color),
task content (title, description, dates, status), instance data (date, status),
and 5 safe preference fields. Zero credentials exported — no OAuth tokens,
session data, encryption keys, passkey credentials, or Google refresh tokens.

### OAuth Token Lifecycle ✅

- Access and refresh tokens encrypted at rest via Fernet (`models.py:137-152`)
- Properties decrypt on access; setters encrypt on write
- `refresh_access_token()` has exponential backoff (3 retries) (`google.py:94-124`)
- Database-level `FOR UPDATE SKIP LOCKED` prevents token refresh races (`gcal.py:140`)
- `client_secret` used only server-side in POST to Google Token endpoint
- Tokens never logged — error messages include only status codes

### Cookie Security ✅

`main.py:154-161` — SessionMiddleware configured correctly:
- `HttpOnly` — implicitly True (Starlette default, hardcoded in source)
- `Secure` — True in production (`https_only=is_production`)
- `SameSite=lax` — prevents cross-site cookie sending for unsafe methods
- 30-day lifetime (`max_age=60*60*24*30`)
- OAuth state cookies: `httponly=True`, `max_age=600` (10 min TTL)

### CORS ✅

No CORS middleware installed — default behavior is deny all cross-origin requests.
CSRF middleware covers all state-changing requests. No API endpoints require
cross-origin access.

### Service Worker ✅

`sw.js` — Cache strategy reviewed:
- Static assets: Cache First (appropriate)
- API requests: Network First with 5-minute TTL (`X-SW-Cached-At` header, lines 169-196)
- HTML pages: Network First (always revalidated)
- Login pages not pre-cached

TTL enforcement prevents stale API data. Legacy cache entries without timestamp are
served but immediately replaced on next network request.

### Background Task Resilience ✅

`recurring.py:136-185` — Materialization loop has:
- 300-second timeout per cycle (circuit breaker)
- Per-user error isolation with try-catch and `continue`
- No unbounded retry queues

`gcal_sync.py:159-166` — Auto-disables sync on persistent calendar errors (403/404/410).
`_AdaptiveThrottle` has max 3 retries before re-raising.

### Migration Safety ✅

All 8 Alembic migrations have working downgrade methods (not `NotImplementedError`).
Revision chain is correct. Foreign key cascades are explicit.

### XSS / Template Injection ✅ (from audit #3, reconfirmed)

Zero `|safe` filters, zero `{% autoescape false %}`, Jinja2 auto-escaping throughout.
JS uses `escapeHtml()` or `textContent` for all user content.

### Multitenancy ✅ (from audit #2, reconfirmed)

28+ queries filter by user_id. Backup import creates records with authenticated user's
ID. No cross-tenant leaks.

### `/api/v1/tasks/all-content` Unbounded Response ✅ (by design)

`tasks.py:399-422` returns all non-archived tasks and domains without pagination.
This is **intentionally unbounded** — the encryption enable/disable flow requires the
client to encrypt/decrypt every record in a single batch operation. Pagination would
break the encryption UX (partial encrypt = data corruption). The endpoint requires
authentication and is called only during one-time encryption toggle operations.
Future audits should not re-flag this.

---

## 4. Agent Findings Assessed and Rejected

Several agent investigations flagged issues that, upon careful analysis, are either
non-issues or already covered:

| Agent Finding | Assessment | Reason |
|---------------|------------|--------|
| Sequential integer IDs enable enumeration | **Non-issue** | All endpoints return 404 for both "not found" and "not owned" (single-query pattern). No timing difference. No information leak. |
| Task completion race condition (last-write-wins) | **Non-issue** | Completing a task is idempotent. Two concurrent completions both succeed and produce identical state. Standard web app behavior without optimistic locking. |
| Domain deletion doesn't prevent linking to archived domain | **Non-issue** | Archived domains still exist in DB. Tasks with archived domain_id display correctly. No orphaned references. |
| Task creation has no idempotency key | **Non-issue** | Rate limiting is the primary control. Duplicate tasks from network retries are a UX issue, not a security or correctness issue. |
| Demo user emails logged in `demo_service.py:87,108` | **Non-issue** | Demo emails are `*@whendoist.local` — synthetic addresses, not real PII. |
| Encryption passphrase recovery | **Already in backlog** | "Encryption passphrase change" (CHANGELOG line 21) |
| No circular parent_id detection on import | **Moot** | parent_id isn't imported (see L4). Even if it were, `_archive_subtasks` only recurses via `parent_id` FK which can't form cycles in single-table self-reference with an auto-increment PK. |
| Failed migrations could partially succeed | **Acceptable** | Alembic wraps migrations in transactions by default. PostgreSQL rolls back on failure. |

---

## 5. Summary

| # | Finding | Severity | In Backlog? | Action |
|---|---------|----------|-------------|--------|
| M1 | Task sheet dead code with CSRF gaps + raw fetch | MEDIUM | No | Fix before activating, or delete |
| M2 | Numeric fields missing range validation | MEDIUM | No | Fix now (small change) |
| L1 | String fields missing max_length vs DB | LOW | No | Fix now (small change) |
| L2 | Collection fields missing size limits | LOW | No | Fix now (small change) |
| L3 | Backup import loses subtask hierarchy | LOW | No | Fix now |

**5 new findings (2 MEDIUM, 3 LOW). Zero HIGH. Zero CRITICAL.**

All audited areas outside these findings are clean. The codebase's security posture
is solid — these are defense-in-depth input validation gaps and a data fidelity bug,
not exploitable vulnerabilities. The task sheet CSRF issue is the highest-priority
item but is currently dead code.

Previous audits: `2026-02-09-recurring-task-audit.md`, `2026-02-09-gcal-sync-audit.md`,
`2026-02-09-pwa-offline-investigation.md`, `2026-02-09-v1-gate-audit-2.md`,
`2026-02-09-v1-gate-audit-3.md`.
